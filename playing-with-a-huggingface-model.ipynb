{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSc7AU66mJSC"
      },
      "source": [
        "##### Copyright 2025 Laurent Brusa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tc6tjo9vmJSE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuC_VSKMcEt6"
      },
      "source": [
        "# Workshop: Play with a huggingface model (Part 1)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1AFSZrjC5aMhtxnbYWh3RYOSjdxd1iPjZ?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A3ged_7WFo6",
        "outputId": "70f07e42-94d7-4ebb-83b8-b599df0eceb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'playing-with-a-huggingface-model'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 19 (delta 3), reused 14 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (19/19), 711.95 KiB | 2.26 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/playing-with-a-huggingface-model\n",
        "!git clone https://github.com/multitudes/playing-with-a-huggingface-model.git\n",
        "# Create the target directory if it doesn't exist\n",
        "!mkdir -p /content/exercise_input/\n",
        "# copy the files with\n",
        "!cp playing-with-a-huggingface-model/exercise_input/function_calling_tests.json /content/exercise_input/\n",
        "!cp playing-with-a-huggingface-model/exercise_input/functions_definition.json /content/exercise_input/\n",
        "!cp playing-with-a-huggingface-model/merges.txt /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_FXjhH-VRl9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Small_LLM Class Documentation\n",
        "\n",
        "Utility class wrapping a lightweight Hugging Face causal language model for fast, \n",
        "low-memory experimentation and inference.\n",
        "\n",
        "This class provides a simplified interface for loading and using causal language models\n",
        "from Hugging Face, with automatic device selection, optimized memory usage, and utility\n",
        "methods for tokenization and inference.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- **model_name** : `str`, default=`\"Qwen/Qwen3-0.6B\"`\n",
        "  - Identifier of the model on the ðŸ¤— Hub. Should be a valid causal language model that can be loaded with AutoModelForCausalLM.\n",
        "\n",
        "- **device** : `str | None`, default=`None`\n",
        "  - Computation device. If None, automatically selects device with priority: MPS (macOS) > CUDA (GPU) > CPU.\n",
        "\n",
        "- **dtype** : `torch.dtype | None`, default=`None`\n",
        "  - Numerical precision for model weights. If None, defaults to float16 for GPU/MPS devices to conserve memory, and float32 for CPU for compatibility.\n",
        "\n",
        "- **trust_remote_code** : `bool`, default=`True`\n",
        "  - Whether to allow custom code from the model repository. Required for some models that include custom modeling code.\n",
        "\n",
        "## Attributes\n",
        "\n",
        "- **_model_name** : `str`\n",
        "  - The name/identifier of the loaded model.\n",
        "\n",
        "- **_device** : `str`\n",
        "  - The device where the model is loaded.\n",
        "\n",
        "- **_dtype** : `torch.dtype`\n",
        "  - The data type used for model weights.\n",
        "\n",
        "- **_tokenizer** : `PreTrainedTokenizer`\n",
        "  - The tokenizer associated with the model.\n",
        "\n",
        "- **_model** : `PreTrainedModel`\n",
        "  - The loaded causal language model in evaluation mode.\n",
        "\n",
        "## Methods\n",
        "\n",
        "- **get_logits_from_input_ids**(`input_ids: list[int]`) â†’ `list[float]`\n",
        "  - Get raw logits for the next token given a sequence of input token IDs.\n",
        "\n",
        "- **get_path_to_vocabulary_json**() â†’ `str`\n",
        "  - Download and return the path to the model's vocabulary JSON file.\n",
        "\n",
        "## Notes\n",
        "\n",
        "- The model is automatically set to evaluation mode and gradients are disabled for all parameters to optimize for inference.\n",
        "- If the tokenizer lacks a pad token, the EOS token is used as the pad token.\n",
        "- For CUDA devices, uses device_map=\"auto\" for efficient multi-GPU handling.\n",
        "\n",
        "## Examples\n",
        "\n",
        "```python\n",
        "# Uses default Qwen model\n",
        "llm = Small_LLM()\n",
        "\n",
        "# Use a specific model with CPU device\n",
        "llm = Small_LLM(\"microsoft/DialoGPT-medium\", device=\"cpu\")\n",
        "\n",
        "# Get logits for next token prediction\n",
        "logits = llm.get_logits_from_input_ids([1, 2, 3, 4])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft7EBMlm1BM8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel, logging\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "logging.set_verbosity_info()\n",
        "\n",
        "\n",
        "class Small_LLM:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"Qwen/Qwen3-0.6B\",\n",
        "        *,\n",
        "        device: str | None = None,\n",
        "        dtype: torch.dtype | None = None,\n",
        "        trust_remote_code: bool = True,\n",
        "    ) -> None:\n",
        "        self._model_name = model_name\n",
        "\n",
        "        # Auto-select device with priority: mps > cuda > cpu\n",
        "        if device is None:\n",
        "            if torch.backends.mps.is_available():\n",
        "                device = \"mps\"\n",
        "            elif torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "            else:\n",
        "                device = \"cpu\"\n",
        "        self._device = device\n",
        "\n",
        "        if dtype is None:\n",
        "            dtype = torch.float16 if self._device in [\"cuda\", \"mps\"] else torch.float32\n",
        "        self._dtype = dtype\n",
        "\n",
        "        # --- load tokenizer & model -------------------------------------------------\n",
        "        self._tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, trust_remote_code=trust_remote_code\n",
        "        )\n",
        "        if self._tokenizer.pad_token_id is None:\n",
        "            # ensure we have a pad token to keep batch helpers happy\n",
        "            self._tokenizer.pad_token_id = self._tokenizer.eos_token_id\n",
        "\n",
        "        self._model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=self._dtype,\n",
        "            device_map=\"auto\" if self._device == \"cuda\" else None,\n",
        "            trust_remote_code=trust_remote_code,\n",
        "        )\n",
        "        self._model.to(self._device).eval()\n",
        "\n",
        "        # switch to inference-only mode\n",
        "        for p in self._model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Public helpers\n",
        "    # -------------------------------------------------------------------------\n",
        "    def get_logits_from_input_ids(self, input_ids: list[int]) -> list[float]:\n",
        "        \"\"\"\n",
        "        Given a list of input token ids, return the raw logits (no softmax) \n",
        "        for the next token as a list of floats.\n",
        "        \"\"\"\n",
        "        input_tensor = torch.tensor([input_ids], device=self._device, dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            out = self._model(input_ids=input_tensor)\n",
        "        # Get logits for the last token in the sequence for the batch (batch size 1)\n",
        "        logits = out.logits[0, -1].tolist()\n",
        "        return [float(x) for x in logits]\n",
        "\n",
        "    def get_path_to_vocabulary_json(self) -> str:\n",
        "        # Download and get paths to specific files\n",
        "        vocab_file_name = self._tokenizer.vocab_files_names.get('vocab_file', \"vocab.json\")\n",
        "        vocab_path = hf_hub_download(\n",
        "            repo_id=self._model_name,\n",
        "            filename=vocab_file_name\n",
        "        )\n",
        "        return vocab_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMZmdSro1GG0"
      },
      "outputs": [],
      "source": [
        "# src/bpe_tokenizer.py\n",
        "import re\n",
        "import json\n",
        "\n",
        "MAX_TOKENS = 150\n",
        "SPECIAL_TOKENS = [\"<|im_start|>\", \"<|im_end|>\", \"<think>\", \"</think>\"]\n",
        "END_TOKEN_ID1 = 3417\n",
        "END_TOKEN_ID2 = 30975\n",
        "MERGES_PATH = \"merges.txt\"\n",
        "SPECIAL_TOKENS = {\n",
        "    \"<|im_start|>\": 151644,\n",
        "    \"<|im_end|>\": 151645,\n",
        "    \"<think>\": 151667,\n",
        "    \"</think>\": 151668,\n",
        "}\n",
        "# These token IDs to signal end of json generation '}}' and '\\\"}}\"\n",
        "\n",
        "\n",
        "def initialize_tokenizer(vocab_path):\n",
        "    \"\"\"\n",
        "    Initialize the BPE tokenizer by loading the vocabulary and merge ranks\n",
        "    from the respective files. The merge_ranks is a dictionary mapping\n",
        "    token pairs to their rank (lower rank means higher priority for merging).\n",
        "    The special tokens are added to the vocabulary if not already present.\n",
        "    The merge.txt file is expected and eventually needs to be downloaded\n",
        "    from the same source as the vocab.json file. It contains the rules for\n",
        "    merging tokens during the BPE tokenization process.\n",
        "    Returns: vocab (dict): Mapping of tokens to their IDs.\n",
        "        merge_ranks (dict): Mapping of token pairs to their merge ranks.\n",
        "    Raises: RuntimeError: If there is an error loading the vocabulary\n",
        "        or merges file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(vocab_path, \"r\") as f:\n",
        "            vocab = json.load(f)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading vocabulary: {e}\")\n",
        "    for tok, tid in SPECIAL_TOKENS.items():\n",
        "        if tok not in vocab:\n",
        "            vocab[tok] = tid\n",
        "    try:\n",
        "        with open(MERGES_PATH, \"r\") as f:\n",
        "            merges = [line.strip().split()\n",
        "                      for line in f if not line.startswith(\"#\")]\n",
        "        merge_ranks = {tuple(merge): i for i, merge in enumerate(merges)}\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Error loading merges.txt file needed for the tokenizer: {e}\")\n",
        "    return vocab, merge_ranks\n",
        "\n",
        "\n",
        "def get_pairs(tokens):\n",
        "    \"\"\"\n",
        "    Return set of adjacent token pairs.\n",
        "    \"\"\"\n",
        "    return {(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)}\n",
        "\n",
        "\n",
        "def preprocess_for_bpe(text):\n",
        "    \"\"\"\n",
        "    Preprocess text for BPE tokenization by replacing spaces,\n",
        "    newlines, and tabs for consistent tokenization.\n",
        "    \"\"\"\n",
        "    text = text.replace(\" \", \"Ä \")\n",
        "    text = text.replace(\"\\n\", \"ÄŠ\")\n",
        "    text = text.replace(\"\\t\", \"Ä‰\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def bpe_tokenize(text, vocab, merge_ranks):\n",
        "    \"\"\"\n",
        "    A simple BPE tokenizer implementation.\n",
        "    This function tokenizes the input text using Byte Pair Encoding (BPE)\n",
        "    based on the provided vocabulary and merge ranks.\n",
        "    There are some special tokens that are needed for the prompt structure\n",
        "    that should be treated as single tokens and not split further like:\n",
        "    151644: <|im_start|>\n",
        "    151645: <|im_end|>\n",
        "    \"\"\"\n",
        "    pattern = \"(\" + \"|\".join(\n",
        "        re.escape(tok) for tok in SPECIAL_TOKENS.keys()) + \")\"\n",
        "    parts = re.split(pattern, text)\n",
        "    tokens = []\n",
        "    for part in parts:\n",
        "        if part in SPECIAL_TOKENS:\n",
        "            tokens.append(part)\n",
        "        else:\n",
        "            tokens.extend(list(preprocess_for_bpe(part)))\n",
        "    while True:\n",
        "        pairs = get_pairs(tokens)\n",
        "        # Find the best pair to merge\n",
        "        min_rank = float('inf')\n",
        "        best_pair = None\n",
        "        for pair in pairs:\n",
        "            if pair in merge_ranks and merge_ranks[pair] < min_rank:\n",
        "                min_rank = merge_ranks[pair]\n",
        "                best_pair = pair\n",
        "        if best_pair is None:\n",
        "            break\n",
        "        # Merge all occurrences of the best pair\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
        "                new_tokens.append(tokens[i] + tokens[i+1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        tokens = new_tokens\n",
        "    return [vocab[token] for token in tokens if token in vocab]\n",
        "\n",
        "\n",
        "def custom_decode(ids, id_to_token):\n",
        "    \"\"\"\n",
        "    Custom decode function to convert token IDs back to text.\n",
        "    There are some special token IDS that we want to skip in the response:\n",
        "    151667: <think>\n",
        "    151668: </think>\n",
        "    See SPECIAL_TOKENS dictionary above for reference.\n",
        "    Those tokens are not included in the vocab dictionary we use for decoding.\n",
        "    \"\"\"\n",
        "    skip_ids = set(SPECIAL_TOKENS.values())\n",
        "    tokens = [id_to_token.get(i, \"<unk>\") for i in ids if i not in skip_ids]\n",
        "    text = \"\".join(tokens)\n",
        "    text = text.replace(\"Ä \", \" \").replace(\"ÄŠ\", \"\\n\").replace(\"Ä‰\", \"\\t\")\n",
        "    print(\"\\n\\nllm output:\", text, end=\"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def create_prompt(user_input: str, tools: str) -> str:\n",
        "    \"\"\"\n",
        "    Create the prompt for the LLM based on user input and available tools.\n",
        "    \"\"\"\n",
        "    system_msg = \"You are a helpful assistant that uses tools. \"\n",
        "    system_msg += \"Based on the user's request, you must call the \"\n",
        "    system_msg += \"appropriate tool with the correct arguments. \"\n",
        "    system_msg += \"You have access to the following tools:\\n\"\n",
        "    system_msg += f\"{tools}\"\n",
        "    system_msg += \"\"\"\n",
        "---\n",
        "Here are some examples:\n",
        "\n",
        "User: Multiply 45 by 11\n",
        "Assistant: {\"fn_name\": \"fn_multiply_numbers\", \"args\": {\"a\": 45.0, \"b\": 11.0}}\n",
        "\n",
        "User: can you reverse the word 'banana'?\n",
        "Assistant: {\"fn_name\": \"fn_reverse_string\", \"args\": {\"s\": \"banana\"}}\n",
        "\n",
        "User: Substitute the digits in the string 'Hello 34 I'm 233 years old' with 'NUMBERS'\n",
        "Assistant: {\"fn_name\": \"fn_substitute_string_with_regex\", \"args\": {\"source_string\": \"Hello 34 I'm 233 years old\", \"regex\": \"\\\\\\\\d+\", \"replacement\": \"NUMBERS\"}}\n",
        "---\n",
        "\n",
        "Now, answer the following request. Only provide the JSON for the tool call.\n",
        "\"\"\"\n",
        "    return (\n",
        "        f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>user\\n{user_input}/no_think<|im_end|>\\n\"\n",
        "        f\"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_answer_ids(llm, input_ids):\n",
        "    \"\"\"\n",
        "    The llmm takes a list of input token ids and generates\n",
        "    a list of logits for the next token at each step.\n",
        "    The next token is chosen as the one with the highest logit,\n",
        "    and appended to the input_ids for the next generation.\n",
        "    At the same time I am interested in collecting the\n",
        "    generated token ids to decode later in answer_ids.\n",
        "    args:\n",
        "        llm: instance of Small_LLM class\n",
        "        input_ids: list of input token ids (integers)\n",
        "    returns: list of generated token ids (integers)\n",
        "    The generation stops when either the maximum number of tokens\n",
        "    is reached or when the end token is generated.\n",
        "    \"\"\"\n",
        "    answer_ids = []\n",
        "    for _ in range(MAX_TOKENS):\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        logits = llm.get_logits_from_input_ids(input_ids)\n",
        "        next_token_id = max(enumerate(logits), key=lambda x: x[1])[0]\n",
        "        input_ids.append(next_token_id)\n",
        "        answer_ids.append(next_token_id)\n",
        "        if (next_token_id == END_TOKEN_ID1 or next_token_id == END_TOKEN_ID2):\n",
        "            break\n",
        "    return answer_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LJkJ5yzn1Lst"
      },
      "outputs": [],
      "source": [
        "# src/schemas.py\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "class FunctionDefinition(BaseModel):\n",
        "    \"\"\"Schema for defining a function signature.\n",
        "\n",
        "    This is what we get from input/functions_definition.json.\n",
        "    This will be passed to the model in the prompt to let it know\n",
        "    what functions are available to call.\n",
        "\n",
        "    Example:\n",
        "        {\n",
        "            \"fn_name\": \"fn_add_numbers\",\n",
        "            \"args_names\": [\"a\", \"b\"],\n",
        "            \"args_types\": {\n",
        "                \"a\": \"float\",\n",
        "                \"b\": \"float\"\n",
        "            },\n",
        "            \"return_type\": \"float\"\n",
        "        }\n",
        "\n",
        "    Attributes:\n",
        "        fn_name (str): Name of the function.\n",
        "        args_names (List[str]): Ordered list of argument names.\n",
        "        args_types (Dict[str, str]): Mapping of argument names to their types.\n",
        "        return_type (str): The return type of the function.\n",
        "    \"\"\"\n",
        "    fn_name: str\n",
        "    args_names: List[str]\n",
        "    args_types: Dict[str, str]\n",
        "    return_type: str\n",
        "\n",
        "\n",
        "class SelectedFunction(BaseModel):\n",
        "    \"\"\"Schema for the function selected by the model to call.\n",
        "\n",
        "    This is what we expect the model to output after being prompted\n",
        "    with a list of available functions and a user prompt.\n",
        "\n",
        "    Attributes:\n",
        "        prompt (str): The original natural-language request.\n",
        "        fn_name (str | None): The name of the function to call.\n",
        "        args (Dict[str, Any]): All required arguments with the correct types.\n",
        "    \"\"\"\n",
        "    prompt: str\n",
        "    fn_name: str | None\n",
        "    args: Dict[str, Any]\n",
        "\n",
        "\n",
        "class ToolParameter(BaseModel):\n",
        "    \"\"\"Represents the parameters for a tool function.\n",
        "\n",
        "    Example:\n",
        "        {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"city\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city to get the weather for\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"city\"]\n",
        "        }\n",
        "\n",
        "    Attributes:\n",
        "        type (str): The type of the parameters object (default: \"object\").\n",
        "        properties (Dict[str, Dict[str, str]]): Properties of the parameters.\n",
        "        required (List[str]): List of required parameter names.\n",
        "    \"\"\"\n",
        "    type: str = \"object\"\n",
        "    properties: Dict[str, Dict[str, str]]\n",
        "    required: List[str] = []\n",
        "\n",
        "\n",
        "class ToolFunction(BaseModel):\n",
        "    \"\"\"Represents a function within a tool.\n",
        "\n",
        "    Example:\n",
        "        {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the weather in a given city\",\n",
        "            \"parameters\": { ... }\n",
        "        }\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The function name.\n",
        "        description (str): The function description.\n",
        "        parameters (ToolParameter): The function parameters.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    parameters: ToolParameter\n",
        "\n",
        "\n",
        "class Tool(BaseModel):\n",
        "    \"\"\"Represents a tool schema for LLM tool selection.\n",
        "\n",
        "    Example:\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": { ... }\n",
        "        }\n",
        "\n",
        "    Attributes:\n",
        "        type (str): The type of the tool (default: \"function\").\n",
        "        function (ToolFunction): The function associated with the tool.\n",
        "    \"\"\"\n",
        "    type: str = \"function\"\n",
        "    function: ToolFunction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zv0hwnr31Oa8"
      },
      "outputs": [],
      "source": [
        "# src/utils.py\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "from typing import List\n",
        "\n",
        "OUTPUT_FILE = \"output/function_calling_name.json\"\n",
        "TOOLS_DEFINITION_FILE = \"exercise_input/functions_definition.json\"\n",
        "THINK_TAG = \"</think>\"\n",
        "\n",
        "\n",
        "def get_functions() -> List[FunctionDefinition]:\n",
        "    \"\"\"\n",
        "    The functions to load are actually the ones defined in the\n",
        "    PATH_TOOLS_DEFINITION file. Those are converted to\n",
        "    FunctionDefinition objects and returned as a list.\n",
        "    Raises: RuntimeError: If there is an error loading the file\n",
        "        or parsing the JSON because the program cannot continue without\n",
        "        a valid functions list.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(TOOLS_DEFINITION_FILE) as f:\n",
        "            functions_raw = json.load(f)\n",
        "        functions = [FunctionDefinition(**fn) for fn in functions_raw]\n",
        "        return functions\n",
        "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "        raise RuntimeError(f\"Error loading functions for tools: {e}\")\n",
        "\n",
        "\n",
        "def get_input_prompts(file: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load the prompts from a JSON file.\n",
        "    Each prompt should be under the \"prompt\" key.\n",
        "    Args:\n",
        "        file (str): Path to the JSON file containing the prompts.\n",
        "    Returns: List[str]: List of prompt strings.\n",
        "    Raises: RuntimeError: If there is an error loading the file\n",
        "        or parsing the JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file) as f:\n",
        "            prompts_raw = json.load(f)\n",
        "        return [pr[\"prompt\"] for pr in prompts_raw]\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(f\"Error: {file} not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise RuntimeError(f\"Error: JSON decode failed for {file}.\")\n",
        "\n",
        "\n",
        "def get_tool_list() -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of FunctionDefinition objects to a JSON string\n",
        "    If the json conversion fails, a RuntimeError is raised because the\n",
        "    program cannot continue without a valid tools list.\n",
        "    \"\"\"\n",
        "    tools = []\n",
        "    functions = get_functions()\n",
        "    for fn in functions:\n",
        "        # Build properties for each argument\n",
        "        properties = {}\n",
        "        for name in fn.args_names:\n",
        "            arg_type = fn.args_types[name]\n",
        "            if arg_type == \"float\":\n",
        "                type_str = \"number\"\n",
        "            elif arg_type == \"str\":\n",
        "                type_str = \"string\"\n",
        "            elif arg_type == \"int\":\n",
        "                type_str = \"integer\"\n",
        "            else:\n",
        "                type_str = \"any\"\n",
        "            properties[name] = {\"type\": type_str}\n",
        "        # Create a readable description from the function name\n",
        "        readable_name = fn.fn_name\n",
        "        if readable_name.startswith(\"fn_\"):\n",
        "            readable_name = readable_name[3:]\n",
        "        description = readable_name.replace('_', ' ') + \" function\"\n",
        "        tool = Tool(\n",
        "            function=ToolFunction(\n",
        "                name=fn.fn_name,\n",
        "                description=description,\n",
        "                parameters=ToolParameter(\n",
        "                    properties=properties,\n",
        "                    required=fn.args_names\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        tools.append(tool.model_dump()) # Use model_dump() instead of dict()\n",
        "    try:\n",
        "        return json.dumps(tools, indent=2)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error converting tools to JSON: {e}\")\n",
        "\n",
        "\n",
        "def enforce_arg_types(fn_name, args, functions_def):\n",
        "    \"\"\"\n",
        "    Sometimes the LLM returns a float as 1 instead of 1.0 for example.\n",
        "    Given a function name in the input requirements\n",
        "    and its arguments as strings, convert the argument\n",
        "    values to the correct types based on the function definitions.\n",
        "    If the function name is not found in the definitions,\n",
        "    or if an argument cannot be converted, it is left as is.\n",
        "    Args:\n",
        "        fn_name (str): The name of the function.\n",
        "        args (dict): The arguments from the LLM output as strings.\n",
        "        functions_def (List[dict]): List of function definitions\n",
        "        (tools) as dicts.\n",
        "    Returns:\n",
        "        dict: The arguments with values converted to the correct types.\n",
        "    \"\"\"\n",
        "    # Find the function definition\n",
        "    fn_def = next((f for f in functions_def if f[\"fn_name\"] == fn_name), None)\n",
        "    if not fn_def:\n",
        "        return args\n",
        "    for arg_name, arg_type in fn_def[\"args_types\"].items():\n",
        "        if arg_name in args:\n",
        "            try:\n",
        "                if arg_type == \"float\":\n",
        "                    # print(f\"Converting arg {arg_name} to float\")\n",
        "                    args[arg_name] = float(args[arg_name])\n",
        "                    # print(f\"Converted arg {arg_name}: {args[arg_name]}\")\n",
        "                elif arg_type == \"int\":\n",
        "                    args[arg_name] = int(args[arg_name])\n",
        "                elif arg_type == \"str\":\n",
        "                    args[arg_name] = str(args[arg_name])\n",
        "                # Add more types as needed\n",
        "            except (ValueError, TypeError):\n",
        "                pass  # Leave as is if conversion fails\n",
        "    return args\n",
        "\n",
        "\n",
        "def extract_json_from_response(\n",
        "        prompt: str,\n",
        "        response: str\n",
        ") -> SelectedFunction | None:\n",
        "    \"\"\"\n",
        "    Extracts and parses a JSON object from the model's full output string.\n",
        "    If the model output does not contain valid JSON, returns an empty\n",
        "    SelectedFunction object with fn_name as an empty string.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The original natural-language request.\n",
        "        response (str): The full output string from the model.\n",
        "    Returns:\n",
        "        SelectedFunction: The parsed SelectedFunction object.\n",
        "    \"\"\"\n",
        "    # First get rid of the think block if it exists\n",
        "    if THINK_TAG in response:\n",
        "        response = response.split(THINK_TAG, 1)[1].strip()\n",
        "    pattern = r'\\{\\s*\"fn_name\":.*?\\}\\s*\\}'\n",
        "    match = re.search(pattern, response, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"No JSON object found in the response.\")\n",
        "        return SelectedFunction(prompt=prompt, fn_name=\"\", args={})\n",
        "    json_str = match.group(0)\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "        fn_name = data.get(\"fn_name\")\n",
        "        args = data.get(\"args\", {})\n",
        "        functions_def = get_functions()\n",
        "        # Convert to dicts for enforce_arg_types\n",
        "        functions_def_dicts = [fn.model_dump() for fn in functions_def]\n",
        "        args = enforce_arg_types(fn_name, args, functions_def_dicts)\n",
        "        print(f\"\\nargs post check: {args}\")\n",
        "        return SelectedFunction(prompt=prompt, fn_name=fn_name, args=args)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON from response: {e}\")\n",
        "        return SelectedFunction(prompt=prompt, fn_name=\"\", args={})\n",
        "\n",
        "\n",
        "def write_output_to_file(output_to_write_to_file):\n",
        "    os.makedirs(\"output\", exist_ok=True)\n",
        "    with open(OUTPUT_FILE, \"w\") as f:\n",
        "        json.dump([o.model_dump() for o in output_to_write_to_file], f) # Use model_dump() instead of dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259UaUFY1RJ_",
        "outputId": "c4da18b4-f39a-4ab4-ed0a-634486cfbcc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"float16\",\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors\n",
            "Instantiating Qwen3ForCausalLM model under default dtype torch.float16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
            "\n",
            "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-0.6B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.95\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Processing prompt: What is the square root of 16?\n",
            "..........................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_get_square_root\", \"args\": {\"a\": 16.0}}\n",
            "Extracted function call: fn_get_square_root with args {'a': 16.0}\n",
            "\n",
            "\n",
            "Processing prompt: Reverse the string 'hello'\n",
            "......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_reverse_string\", \"args\": {\"s\": \"hello\"}}\n",
            "Extracted function call: fn_reverse_string with args {'s': 'hello'}\n",
            "\n",
            "\n",
            "Processing prompt: Reverse the string 'world'\n",
            "......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_reverse_string\", \"args\": {\"s\": \"world\"}}\n",
            "Extracted function call: fn_reverse_string with args {'s': 'world'}\n",
            "\n",
            "\n",
            "Processing prompt: Substitute the digits in the string 'Hello 34 I'm 233 years old' with 'NUMBERS'\n",
            "..................................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_substitute_string_with_regex\", \"args\": {\"source_string\": \"Hello 34 I'm 233 years old\", \"regex\": \"\\\\d+\", \"replacement\": \"NUMBERS\"}}\n",
            "Extracted function call: fn_substitute_string_with_regex with args {'source_string': \"Hello 34 I'm 233 years old\", 'regex': '\\\\d+', 'replacement': 'NUMBERS'}\n",
            "\n",
            "\n",
            "Processing prompt: Replace all vowels in 'Programming is fun' with asterisks\n",
            "..........................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_substitute_string_with_regex\", \"args\": {\"source_string\": \"Programming is fun\", \"regex\": \"\\\\w|aeiou\", \"replacement\": \"*\"}}\n",
            "Extracted function call: fn_substitute_string_with_regex with args {'source_string': 'Programming is fun', 'regex': '\\\\w|aeiou', 'replacement': '*'}\n",
            "\n",
            "\n",
            "Processing prompt: Substitute the word 'cat' with 'dog' in 'The cat sat on the mat with another cat'\n",
            ".................................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_substitute_string_with_regex\", \"args\": {\"source_string\": \"The cat sat on the mat with another cat\", \"regex\": \"\\\\bcat\\\\b\", \"replacement\": \"dog\"}}\n",
            "Extracted function call: fn_substitute_string_with_regex with args {'source_string': 'The cat sat on the mat with another cat', 'regex': '\\\\bcat\\\\b', 'replacement': 'dog'}\n",
            "\n",
            "\n",
            "Processing prompt: What is the sum of 2 and 3?\n",
            "............................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_add_numbers\", \"args\": {\"a\": 2, \"b\": 3}}\n",
            "Extracted function call: fn_add_numbers with args {'a': 2.0, 'b': 3.0}\n",
            "\n",
            "\n",
            "Processing prompt: What is the sum of 265 and 345?\n",
            "....................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_add_numbers\", \"args\": {\"a\": 265.0, \"b\": 345.0}}\n",
            "Extracted function call: fn_add_numbers with args {'a': 265.0, 'b': 345.0}\n",
            "\n",
            "\n",
            "Processing prompt: Is 4 an even number?\n",
            "......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_is_even\", \"args\": {\"n\": 4}}\n",
            "Extracted function call: fn_is_even with args {'n': 4}\n",
            "\n",
            "\n",
            "Processing prompt: Is 7 an even number?\n",
            "......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_is_even\", \"args\": {\"n\": 7}}\n",
            "Extracted function call: fn_is_even with args {'n': 7}\n",
            "\n",
            "\n",
            "Processing prompt: What is the product of 3 and 5?\n",
            "................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_multiply_numbers\", \"args\": {\"a\": 3.0, \"b\": 5.0}}\n",
            "Extracted function call: fn_multiply_numbers with args {'a': 3.0, 'b': 5.0}\n",
            "\n",
            "\n",
            "Processing prompt: What is the product of 12 and 4?\n",
            ".................................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_multiply_numbers\", \"args\": {\"a\": 12.0, \"b\": 4.0}}\n",
            "Extracted function call: fn_multiply_numbers with args {'a': 12.0, 'b': 4.0}\n",
            "\n",
            "\n",
            "Processing prompt: Greet shrek\n",
            ".......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_greet\", \"args\": {\"name\": \"shrek\"}}\n",
            "Extracted function call: fn_greet with args {'name': 'shrek'}\n",
            "\n",
            "\n",
            "Processing prompt: Greet john\n",
            "......................\n",
            "\n",
            "llm output: \n",
            "\n",
            "\n",
            "\n",
            "{\"fn_name\": \"fn_greet\", \"args\": {\"name\": \"john\"}}\n",
            "Extracted function call: fn_greet with args {'name': 'john'}\n"
          ]
        }
      ],
      "source": [
        "# src/__main__.py\n",
        "\n",
        "\n",
        "INPUT_FILE = \"exercise_input/function_calling_tests.json\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Main entry point for function-calling LLM pipeline.\n",
        "\n",
        "Args:\n",
        "    input_file (str, optional): Path to the prompts JSON file. Defaults to\n",
        "        \"exercise_input/function_calling_tests.json\".\n",
        "\"\"\"\n",
        "try:\n",
        "    llm = Small_LLM()\n",
        "    vocab_path = llm.get_path_to_vocabulary_json()\n",
        "    vocab, merge_ranks = initialize_tokenizer(vocab_path)\n",
        "    outputs = []\n",
        "    tools = get_tool_list()\n",
        "    # Reverse the vocab dict for ID to token lookup\n",
        "    id_to_token = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    for user_prompt in get_input_prompts(INPUT_FILE):\n",
        "        print(f\"\\n\\nProcessing prompt: {user_prompt}\")\n",
        "        prompt = create_prompt(user_prompt, tools)\n",
        "        # input_ids = llm._encode(final_prompt).tolist()[0]\n",
        "        input_ids = bpe_tokenize(\n",
        "            prompt, vocab=vocab, merge_ranks=merge_ranks)\n",
        "        answer_ids = get_answer_ids(llm, input_ids)\n",
        "        # llm_output = llm._decode(answer_ids)\n",
        "        llm_output = custom_decode(answer_ids, id_to_token)\n",
        "        result = extract_json_from_response(\n",
        "            user_prompt, llm_output)\n",
        "        outputs.append(result)\n",
        "\n",
        "    write_output_to_file(outputs)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Fatal error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq4pRVgI-6Mq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
